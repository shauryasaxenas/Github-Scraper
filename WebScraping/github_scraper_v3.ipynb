{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff4a42c-4bad-4e36-b437-920906e23082",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install all necessary libraries, update if necessary\n",
    "!pip install requests --upgrade --quiet\n",
    "!pip install beautifulsoup4 --upgrade --quiet\n",
    "!pip install pandas --upgrade --quiet\n",
    "!pip install selenium --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fae78c50-8f52-44b8-82ef-6188894543e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import os\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f6fa32d-0e59-4c0c-99a5-2a0f0ef7f840",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_url = 'https://github.com/topics' # Website we are scraping\n",
    "base_url = 'https://github.com' # Base URL we will add website extention to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28df3846-fe16-4883-9134-c9ef7b18c5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Safari() # Initialize Safari Driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66ede17b-07fe-471a-8f05-1853049bbc53",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    driver.get(topics_url)\n",
    "    \n",
    "    for i in range(5):\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.XPATH, \"//button[contains(@class, 'ajax-pagination-btn')]\"))\n",
    "        )\n",
    "        \n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    \n",
    "        # Wait until the button is clickable\n",
    "        button_element = WebDriverWait(driver, 10).until(\n",
    "            EC.element_to_be_clickable((By.XPATH, \"//button[contains(@class, 'ajax-pagination-btn')]\"))\n",
    "        )\n",
    "        \n",
    "        button_element.click()\n",
    "    \n",
    "        time.sleep(1)\n",
    "\n",
    "\n",
    "    page_contents = driver.page_source\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "\n",
    "finally:\n",
    "    driver.quit()  # Always close the driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "988a664b-ef0b-4070-af9f-86a571d6af98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse page contents using the html parser in Beautiful Soup\n",
    "doc = BeautifulSoup(page_contents, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f8c21b3-876e-4f33-85db-a65f60ec7a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_star_count(stars_str):\n",
    "    \"\"\" Parses a star count string and converts it to an integer.\n",
    "\n",
    "    This function takes a string representing a star count, which may include a 'k' suffix\n",
    "    to denote thousands (e.g., .145k'). If the 'k' is present, it converts the numeric\n",
    "    portion to a float, multiplies by 1000, and returns the integer value. If no 'k' is\n",
    "    present, it directly converts the string to an integer. \n",
    "\n",
    "    Args: \n",
    "        stars_str (str): The star count as a string (e.g., '145k', '300').\n",
    "\n",
    "    Returns:\n",
    "        int: The numerical value of the star count (e.g., 145000, 300). \n",
    "    \"\"\"\n",
    "    stars_str = stars_str.strip()\n",
    "    if stars_str[-1] == 'k':\n",
    "        return int(float(stars_str[:-1]) * 1000)\n",
    "    return int(stars_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1cf6aa28-23e2-428e-882e-05eeb4095a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_page(topic_url):\n",
    "    \"\"\" Fetches and parses the HTML document of a GitHub topic page.\n",
    "\n",
    "    Args: \n",
    "        topic_url (str): The URL of the GitHub topic page.  \n",
    "\n",
    "    Returns:\n",
    "        BeautifulSoup: Parsed HTML document of the topic page.\n",
    "\n",
    "    Raises: \n",
    "        Exception: If the page fails to load. \n",
    "    \"\"\"\n",
    "    # Downlaod the page \n",
    "    response = requests.get(topic_url)\n",
    "    \n",
    "    # Check successful response\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f'Failed to load page {topic_url}')\n",
    "        \n",
    "    # Parse using Beautiful Soup\n",
    "    topic_doc = BeautifulSoup(response.text, 'html.parser')\n",
    "    return topic_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "720a5b6d-62b3-4ae0-b82e-3d59af0ad955",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_repo_info(h1_tag, star_tag):\n",
    "    \"\"\" Extracts repository information from the topic page.\n",
    "\n",
    "    Args:\n",
    "        h1_tag (BeautifulSoup tag): The h1 tag that contains the repository details. \n",
    "        star_tag (BeautifulSoup tag): The span tag that contains the star count.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (username, repository name, star count, repository URL).\n",
    "    \"\"\"\n",
    "    a_tags = h1_tag.find_all('a')\n",
    "    username = a_tags[0].text.strip()\n",
    "    repo_name = a_tags[1].text.strip()\n",
    "    repo_url = base_url + a_tags[1]['href']\n",
    "    stars = parse_star_count(star_tag.text.strip())\n",
    "    \n",
    "    return username, repo_name, stars, repo_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d66d898a-22fd-4e44-9b22-d80ca0fd1061",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_repos(topic_doc):\n",
    "    \"\"\" Extracts repository details from a GitHub topic page. \n",
    "\n",
    "    Args:\n",
    "        topic_doc (BeautifulSoup): Parsed the HTML document of the topic page.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing repository details (username, name, stars, URL).\n",
    "    \"\"\"\n",
    "    # Get the h1 tags containing repo title, repo URL, and username\n",
    "    h1_selection_class = 'f3 color-fg-muted text-normal lh-condensed'\n",
    "    repo_tags = topic_doc.find_all('h3', {'class': h1_selection_class})\n",
    "    \n",
    "    # Get star tags\n",
    "    star_tags = topic_doc.find_all('span', {'class': 'Counter js-social-count'})\n",
    "\n",
    "    topic_repos_dict = {\n",
    "        'username' : [],\n",
    "        'repo_name' : [],\n",
    "        'stars': [],\n",
    "        'repo_url' : []\n",
    "    }\n",
    "    \n",
    "    # Get repo info\n",
    "    for i in range(len(repo_tags)):\n",
    "        repo_info = get_repo_info(repo_tags[i], star_tags[i])\n",
    "        topic_repos_dict['username'].append(repo_info[0])\n",
    "        topic_repos_dict['repo_name'].append(repo_info[1])\n",
    "        topic_repos_dict['stars'].append(repo_info[2])\n",
    "        topic_repos_dict['repo_url'].append(repo_info[3])\n",
    "\n",
    "    return pd.DataFrame(topic_repos_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b5d503ef-55aa-4e69-a685-2b6299983d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_topic(topic_url, path):\n",
    "    \"\"\" Scrapes repositories from a GitHub topic page and saves them as a .csv file. \n",
    "\n",
    "    Args:\n",
    "    topic_url (str): The URL of the GitHub topic page. \n",
    "    path (str): The file path to save the scraped data. \n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    if os.path.exists(path):\n",
    "        print(f\"The file {path} already exists. Skipping...\")\n",
    "        return\n",
    "    topic_df = get_topic_repos(get_topic_page(topic_url))\n",
    "    \n",
    "    topic_df.to_csv(path, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "25c8ecf2-1f21-493d-a33d-3a0eb421be1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_titles(doc):\n",
    "    \"\"\" Extracts topic titles from the GitHub topics page.\n",
    "\n",
    "    Args:\n",
    "        doc (BeautifulSoup): Parsed HTML document of the topics page. \n",
    "\n",
    "    Returns:\n",
    "        list: A list of topic titles.\n",
    "    \"\"\"\n",
    "    selection_class = 'f3 lh-condensed mb-0 mt-1 Link--primary'\n",
    "    topic_title_tags = doc.find_all('p', {'class' : selection_class})\n",
    "    topic_titles = []\n",
    "    for tag in topic_title_tags:\n",
    "        topic_titles.append(tag.text)\n",
    "    return topic_titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a4cae779-d3f6-42b7-96aa-549512d7e6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_descs(doc):\n",
    "    \"\"\" Extracts the topic descriptions from the GitHub topics page.\n",
    "\n",
    "    Args:\n",
    "        doc (BeautifulSoup): Parsed HTML document of the topics page.\n",
    "        \n",
    "    Returns:\n",
    "        list: A list of topic descriptions. \n",
    "    \"\"\"\n",
    "    desc_selector = 'f5 color-fg-muted mb-0 mt-1'\n",
    "    topic_desc_tags = doc.find_all('p', {'class', desc_selector})\n",
    "    \n",
    "    topic_descs = []\n",
    "    for tag in topic_desc_tags:\n",
    "        topic_descs.append(tag.text.strip())\n",
    "    return topic_descs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "40db1921-eb04-4a0d-9fc0-5b4b5807f06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_urls(doc):\n",
    "    \"\"\" Extracts topic URLs from the GitHub topics page. \n",
    "\n",
    "    Args:\n",
    "        doc (BeautifulSoup): Parsed HTML document of the topics page.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of GitHub topic URLs.\n",
    "    \"\"\"\n",
    "    topic_link_tags = doc.find_all('a', {'class': 'no-underline flex-1 d-flex flex-column'})\n",
    "    topic_urls = []\n",
    "    base_url = 'https://github.com'\n",
    "    \n",
    "    for tag in topic_link_tags:\n",
    "        topic_urls.append(base_url + tag['href'])\n",
    "        \n",
    "    return topic_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "55afa89c-c5df-4123-b988-1328c2bc7e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_topics():\n",
    "    \"\"\" Scrapes all GitHub topics and their metadata from the topics page.\n",
    "\n",
    "    Args:\n",
    "        None\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing topic titles, descriptions, and URLs. \n",
    "\n",
    "    Raises:\n",
    "        Exception: If the topics page fails to load. \n",
    "    \"\"\"\n",
    "    topics_url = 'https://github.com/topics'\n",
    "    response = requests.get(topics_url)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f'Failed to load page {topic_url}')\n",
    "    topics_dict = {\n",
    "        'title': get_topic_titles(doc), \n",
    "        'description': get_topic_descs(doc),\n",
    "        'url': get_topic_urls(doc)\n",
    "    }\n",
    "    return pd.DataFrame(topics_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "99a92eaa-93a0-43c0-999c-d184aab550f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_topics_repos():\n",
    "    \"\"\" Scrapes repositories from all topics on GitHub and saves them as .csv files. \n",
    "\n",
    "    Args:\n",
    "        None\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    print('Scraping list of topics:')\n",
    "    topics_df = scrape_topics()\n",
    "\n",
    "    os.makedirs('data_v3', exist_ok=True)\n",
    "    \n",
    "    for index, row in topics_df.iterrows():\n",
    "        print(f'Scraping top repos for \"{row['title']}\"')\n",
    "        scrape_topic(row['url'], 'data_v3/{}.csv'.format(row['title']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4fdd12ab-e9d0-4ab6-ab65-f74703e408d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping list of topics:\n",
      "Scraping top repos for \"3D\"\n",
      "The file data_v3/3D.csv already exists. Skipping...\n",
      "Scraping top repos for \"Ajax\"\n",
      "The file data_v3/Ajax.csv already exists. Skipping...\n",
      "Scraping top repos for \"Algorithm\"\n",
      "The file data_v3/Algorithm.csv already exists. Skipping...\n",
      "Scraping top repos for \"Amp\"\n",
      "The file data_v3/Amp.csv already exists. Skipping...\n",
      "Scraping top repos for \"Android\"\n",
      "The file data_v3/Android.csv already exists. Skipping...\n",
      "Scraping top repos for \"Angular\"\n",
      "The file data_v3/Angular.csv already exists. Skipping...\n",
      "Scraping top repos for \"Ansible\"\n",
      "The file data_v3/Ansible.csv already exists. Skipping...\n",
      "Scraping top repos for \"API\"\n",
      "The file data_v3/API.csv already exists. Skipping...\n",
      "Scraping top repos for \"Arduino\"\n",
      "The file data_v3/Arduino.csv already exists. Skipping...\n",
      "Scraping top repos for \"ASP.NET\"\n",
      "Scraping top repos for \"Awesome Lists\"\n",
      "Scraping top repos for \"Amazon Web Services\"\n",
      "Scraping top repos for \"Azure\"\n",
      "Scraping top repos for \"Babel\"\n",
      "Scraping top repos for \"Bash\"\n",
      "Scraping top repos for \"Bitcoin\"\n",
      "Scraping top repos for \"Bootstrap\"\n",
      "Scraping top repos for \"Bot\"\n",
      "Scraping top repos for \"C\"\n",
      "Scraping top repos for \"Chrome\"\n",
      "Scraping top repos for \"Chrome extension\"\n",
      "Scraping top repos for \"Command-line interface\"\n",
      "Scraping top repos for \"Clojure\"\n",
      "Scraping top repos for \"Code quality\"\n",
      "Scraping top repos for \"Code review\"\n",
      "Scraping top repos for \"Compiler\"\n",
      "Scraping top repos for \"Continuous integration\"\n",
      "Scraping top repos for \"C++\"\n",
      "Scraping top repos for \"Cryptocurrency\"\n",
      "Scraping top repos for \"Crystal\"\n",
      "Scraping top repos for \"C#\"\n",
      "Scraping top repos for \"CSS\"\n",
      "Scraping top repos for \"Data structures\"\n",
      "Scraping top repos for \"Data visualization\"\n",
      "Scraping top repos for \"Database\"\n",
      "Scraping top repos for \"Deep learning\"\n",
      "Scraping top repos for \"Dependency management\"\n",
      "Scraping top repos for \"Deployment\"\n",
      "Scraping top repos for \"Django\"\n",
      "Scraping top repos for \"Docker\"\n",
      "Scraping top repos for \"Documentation\"\n",
      "Scraping top repos for \".NET\"\n",
      "Scraping top repos for \"Electron\"\n",
      "Scraping top repos for \"Elixir\"\n",
      "Scraping top repos for \"Emacs\"\n",
      "Scraping top repos for \"Ember\"\n",
      "Scraping top repos for \"Emoji\"\n",
      "Scraping top repos for \"Emulator\"\n",
      "Scraping top repos for \"ESLint\"\n",
      "Scraping top repos for \"Ethereum\"\n",
      "Scraping top repos for \"Express\"\n",
      "Scraping top repos for \"Firebase\"\n",
      "Scraping top repos for \"Firefox\"\n",
      "Scraping top repos for \"Flask\"\n",
      "Scraping top repos for \"Font\"\n",
      "Scraping top repos for \"Framework\"\n",
      "Scraping top repos for \"Front end\"\n",
      "Scraping top repos for \"Game engine\"\n",
      "Scraping top repos for \"Git\"\n",
      "Scraping top repos for \"GitHub API\"\n",
      "Scraping top repos for \"Go\"\n",
      "Scraping top repos for \"Google\"\n",
      "Scraping top repos for \"Gradle\"\n",
      "Scraping top repos for \"GraphQL\"\n",
      "Scraping top repos for \"Gulp\"\n",
      "Scraping top repos for \"Hacktoberfest\"\n",
      "Scraping top repos for \"Haskell\"\n",
      "Scraping top repos for \"Homebrew\"\n",
      "Scraping top repos for \"Homebridge\"\n",
      "Scraping top repos for \"HTML\"\n",
      "Scraping top repos for \"HTTP\"\n",
      "Scraping top repos for \"Icon font\"\n",
      "Scraping top repos for \"iOS\"\n",
      "Scraping top repos for \"IPFS\"\n",
      "Scraping top repos for \"Java\"\n",
      "Scraping top repos for \"JavaScript\"\n",
      "Scraping top repos for \"Jekyll\"\n",
      "Scraping top repos for \"jQuery\"\n",
      "Scraping top repos for \"JSON\"\n",
      "Scraping top repos for \"The Julia Language\"\n",
      "Scraping top repos for \"Jupyter Notebook\"\n",
      "Scraping top repos for \"Koa\"\n",
      "Scraping top repos for \"Kotlin\"\n",
      "Scraping top repos for \"Kubernetes\"\n",
      "Scraping top repos for \"Laravel\"\n",
      "Scraping top repos for \"LaTeX\"\n",
      "Scraping top repos for \"Library\"\n",
      "Scraping top repos for \"Linux\"\n",
      "Scraping top repos for \"Localization (l10n)\"\n",
      "Scraping top repos for \"Lua\"\n",
      "Scraping top repos for \"Machine learning\"\n",
      "Scraping top repos for \"macOS\"\n",
      "Scraping top repos for \"Markdown\"\n",
      "Scraping top repos for \"Mastodon\"\n",
      "Scraping top repos for \"Material Design\"\n",
      "Scraping top repos for \"MATLAB\"\n",
      "Scraping top repos for \"Maven\"\n",
      "Scraping top repos for \"Minecraft\"\n",
      "Scraping top repos for \"Mobile\"\n",
      "Scraping top repos for \"Monero\"\n",
      "Scraping top repos for \"MongoDB\"\n",
      "Scraping top repos for \"Mongoose\"\n",
      "Scraping top repos for \"Monitoring\"\n",
      "Scraping top repos for \"MvvmCross\"\n",
      "Scraping top repos for \"MySQL\"\n",
      "Scraping top repos for \"NativeScript\"\n",
      "Scraping top repos for \"Nim\"\n",
      "Scraping top repos for \"Natural language processing\"\n",
      "Scraping top repos for \"Node.js\"\n",
      "Scraping top repos for \"NoSQL\"\n",
      "Scraping top repos for \"npm\"\n",
      "Scraping top repos for \"Objective-C\"\n",
      "Scraping top repos for \"OpenGL\"\n",
      "Scraping top repos for \"Operating system\"\n",
      "Scraping top repos for \"P2P\"\n",
      "Scraping top repos for \"Package manager\"\n",
      "Scraping top repos for \"Parsing\"\n",
      "Scraping top repos for \"Perl\"\n",
      "Scraping top repos for \"Phaser\"\n",
      "Scraping top repos for \"PHP\"\n",
      "Scraping top repos for \"PICO-8\"\n",
      "Scraping top repos for \"Pixel Art\"\n",
      "Scraping top repos for \"PostgreSQL\"\n",
      "Scraping top repos for \"Project management\"\n",
      "Scraping top repos for \"Publishing\"\n",
      "Scraping top repos for \"PWA\"\n",
      "Scraping top repos for \"Python\"\n",
      "Scraping top repos for \"Qt\"\n",
      "Scraping top repos for \"R\"\n",
      "Scraping top repos for \"Rails\"\n",
      "Scraping top repos for \"Raspberry Pi\"\n",
      "Scraping top repos for \"Ratchet\"\n",
      "Scraping top repos for \"React\"\n",
      "Scraping top repos for \"React Native\"\n",
      "Scraping top repos for \"ReactiveUI\"\n",
      "Scraping top repos for \"Redux\"\n",
      "Scraping top repos for \"REST API\"\n",
      "Scraping top repos for \"Ruby\"\n",
      "Scraping top repos for \"Rust\"\n",
      "Scraping top repos for \"Sass\"\n",
      "Scraping top repos for \"Scala\"\n",
      "Scraping top repos for \"scikit-learn\"\n",
      "Scraping top repos for \"Software-defined networking\"\n",
      "Scraping top repos for \"Security\"\n",
      "Scraping top repos for \"Server\"\n",
      "Scraping top repos for \"Serverless\"\n",
      "Scraping top repos for \"Shell\"\n",
      "Scraping top repos for \"Sketch\"\n",
      "Scraping top repos for \"SpaceVim\"\n",
      "Scraping top repos for \"Spring Boot\"\n",
      "Scraping top repos for \"SQL\"\n",
      "Scraping top repos for \"Storybook\"\n",
      "Scraping top repos for \"Support\"\n",
      "Scraping top repos for \"Swift\"\n",
      "Scraping top repos for \"Symfony\"\n",
      "Scraping top repos for \"Telegram\"\n",
      "Scraping top repos for \"Tensorflow\"\n",
      "Scraping top repos for \"Terminal\"\n",
      "Scraping top repos for \"Terraform\"\n",
      "Scraping top repos for \"Testing\"\n",
      "Scraping top repos for \"Twitter\"\n",
      "Scraping top repos for \"TypeScript\"\n",
      "Scraping top repos for \"Ubuntu\"\n",
      "Scraping top repos for \"Unity\"\n"
     ]
    }
   ],
   "source": [
    "# Starts the web scraping process for all topics and repositories \n",
    "scrape_topics_repos()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
