{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ff4a42c-4bad-4e36-b437-920906e23082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: pip\n",
      "zsh:1: command not found: pip\n",
      "zsh:1: command not found: pip\n",
      "zsh:1: command not found: pip\n"
     ]
    }
   ],
   "source": [
    "# Install all necessary libraries, update if necessary\n",
    "!pip install requests --upgrade --quiet\n",
    "!pip install beautifulsoup4 --upgrade --quiet\n",
    "!pip install pandas --upgrade --quiet\n",
    "!pip install selenium --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fae78c50-8f52-44b8-82ef-6188894543e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import os\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f6fa32d-0e59-4c0c-99a5-2a0f0ef7f840",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_url = 'https://github.com/topics' # Website we are scraping\n",
    "base_url = 'https://github.com' # Base URL we will add website extention to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28df3846-fe16-4883-9134-c9ef7b18c5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Safari() # Initialize Safari Driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66ede17b-07fe-471a-8f05-1853049bbc53",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Driver opens topics page\n",
    "    driver.get(topics_url)\n",
    "\n",
    "    for i in range(5):\n",
    "        # Driver waits until the buttom element is found\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.XPATH, \"//button[contains(@class, 'ajax-pagination-btn')]\"))\n",
    "        )\n",
    "        \n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    \n",
    "        # Wait until the button element is clickable\n",
    "        button_element = WebDriverWait(driver, 10).until(\n",
    "            EC.element_to_be_clickable((By.XPATH, \"//button[contains(@class, 'ajax-pagination-btn')]\"))\n",
    "        )\n",
    "\n",
    "        # Click the button\n",
    "        button_element.click()\n",
    "\n",
    "    # Store updated html file\n",
    "    page_contents = driver.page_source\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "\n",
    "finally:\n",
    "    driver.quit()  # Always close the driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "988a664b-ef0b-4070-af9f-86a571d6af98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse page contents using the html parser in Beautiful Soup\n",
    "doc = BeautifulSoup(page_contents, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f8c21b3-876e-4f33-85db-a65f60ec7a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_star_count(stars_str):\n",
    "    \"\"\" Parses a star count string and converts it to an integer.\n",
    "\n",
    "    This function takes a string representing a star count, which may include a 'k' suffix\n",
    "    to denote thousands (e.g., .145k'). If the 'k' is present, it converts the numeric\n",
    "    portion to a float, multiplies by 1000, and returns the integer value. If no 'k' is\n",
    "    present, it directly converts the string to an integer. \n",
    "\n",
    "    Args: \n",
    "        stars_str (str): The star count as a string (e.g., '145k', '300').\n",
    "\n",
    "    Returns:\n",
    "        int: The numerical value of the star count (e.g., 145000, 300). \n",
    "    \"\"\"\n",
    "    stars_str = stars_str.strip()\n",
    "    if stars_str[-1] == 'k':\n",
    "        return int(float(stars_str[:-1]) * 1000)\n",
    "    return int(stars_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1cf6aa28-23e2-428e-882e-05eeb4095a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_page(topic_url):\n",
    "    \"\"\" Fetches and parses the HTML document of a GitHub topic page.\n",
    "\n",
    "    Args: \n",
    "        topic_url (str): The URL of the GitHub topic page.  \n",
    "\n",
    "    Returns:\n",
    "        BeautifulSoup: Parsed HTML document of the topic page.\n",
    "\n",
    "    Raises: \n",
    "        Exception: If the page fails to load. \n",
    "    \"\"\"\n",
    "    # Downlaod the page \n",
    "    response = requests.get(topic_url)\n",
    "    \n",
    "    # Check successful response\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f'Failed to load page {topic_url}')\n",
    "        \n",
    "    # Parse using Beautiful Soup\n",
    "    topic_doc = BeautifulSoup(response.text, 'html.parser')\n",
    "    return topic_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "720a5b6d-62b3-4ae0-b82e-3d59af0ad955",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_repo_info(h1_tag, star_tag):\n",
    "    \"\"\" Extracts repository information from the topic page.\n",
    "\n",
    "    Args:\n",
    "        h1_tag (BeautifulSoup tag): The h1 tag that contains the repository details. \n",
    "        star_tag (BeautifulSoup tag): The span tag that contains the star count.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (username, repository name, star count, repository URL).\n",
    "    \"\"\"\n",
    "    a_tags = h1_tag.find_all('a')\n",
    "    username = a_tags[0].text.strip()\n",
    "    repo_name = a_tags[1].text.strip()\n",
    "    repo_url = base_url + a_tags[1]['href']\n",
    "    stars = parse_star_count(star_tag.text.strip())\n",
    "    \n",
    "    return username, repo_name, stars, repo_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d66d898a-22fd-4e44-9b22-d80ca0fd1061",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_repos(topic_doc):\n",
    "    \"\"\" Extracts repository details from a GitHub topic page. \n",
    "\n",
    "    Args:\n",
    "        topic_doc (BeautifulSoup): Parsed the HTML document of the topic page.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing repository details (username, name, stars, URL).\n",
    "    \"\"\"\n",
    "    # Get the h1 tags containing repo title, repo URL, and username\n",
    "    h1_selection_class = 'f3 color-fg-muted text-normal lh-condensed'\n",
    "    repo_tags = topic_doc.find_all('h3', {'class': h1_selection_class})\n",
    "    \n",
    "    # Get star tags\n",
    "    star_tags = topic_doc.find_all('span', {'class': 'Counter js-social-count'})\n",
    "\n",
    "    topic_repos_dict = {\n",
    "        'username' : [],\n",
    "        'repo_name' : [],\n",
    "        'stars': [],\n",
    "        'repo_url' : []\n",
    "    }\n",
    "    \n",
    "    # Get repo info\n",
    "    for i in range(len(repo_tags)):\n",
    "        repo_info = get_repo_info(repo_tags[i], star_tags[i])\n",
    "        topic_repos_dict['username'].append(repo_info[0])\n",
    "        topic_repos_dict['repo_name'].append(repo_info[1])\n",
    "        topic_repos_dict['stars'].append(repo_info[2])\n",
    "        topic_repos_dict['repo_url'].append(repo_info[3])\n",
    "\n",
    "    return pd.DataFrame(topic_repos_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b5d503ef-55aa-4e69-a685-2b6299983d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_topic(topic_url, path):\n",
    "    \"\"\" Scrapes repositories from a GitHub topic page and saves them as a .csv file. \n",
    "\n",
    "    Args:\n",
    "    topic_url (str): The URL of the GitHub topic page. \n",
    "    path (str): The file path to save the scraped data. \n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    if os.path.exists(path):\n",
    "        print(f\"The file {path} already exists. Skipping...\")\n",
    "        return\n",
    "    topic_df = get_topic_repos(get_topic_page(topic_url))\n",
    "    \n",
    "    topic_df.to_csv(path, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "25c8ecf2-1f21-493d-a33d-3a0eb421be1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_titles(doc):\n",
    "    \"\"\" Extracts topic titles from the GitHub topics page.\n",
    "\n",
    "    Args:\n",
    "        doc (BeautifulSoup): Parsed HTML document of the topics page. \n",
    "\n",
    "    Returns:\n",
    "        list: A list of topic titles.\n",
    "    \"\"\"\n",
    "    selection_class = 'f3 lh-condensed mb-0 mt-1 Link--primary'\n",
    "    topic_title_tags = doc.find_all('p', {'class' : selection_class})\n",
    "    topic_titles = []\n",
    "    for tag in topic_title_tags:\n",
    "        topic_titles.append(tag.text)\n",
    "    return topic_titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a4cae779-d3f6-42b7-96aa-549512d7e6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_descs(doc):\n",
    "    \"\"\" Extracts the topic descriptions from the GitHub topics page.\n",
    "\n",
    "    Args:\n",
    "        doc (BeautifulSoup): Parsed HTML document of the topics page.\n",
    "        \n",
    "    Returns:\n",
    "        list: A list of topic descriptions. \n",
    "    \"\"\"\n",
    "    desc_selector = 'f5 color-fg-muted mb-0 mt-1'\n",
    "    topic_desc_tags = doc.find_all('p', {'class', desc_selector})\n",
    "    \n",
    "    topic_descs = []\n",
    "    for tag in topic_desc_tags:\n",
    "        topic_descs.append(tag.text.strip())\n",
    "    return topic_descs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "40db1921-eb04-4a0d-9fc0-5b4b5807f06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_urls(doc):\n",
    "    \"\"\" Extracts topic URLs from the GitHub topics page. \n",
    "\n",
    "    Args:\n",
    "        doc (BeautifulSoup): Parsed HTML document of the topics page.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of GitHub topic URLs.\n",
    "    \"\"\"\n",
    "    topic_link_tags = doc.find_all('a', {'class': 'no-underline flex-1 d-flex flex-column'})\n",
    "    topic_urls = []\n",
    "    base_url = 'https://github.com'\n",
    "    \n",
    "    for tag in topic_link_tags:\n",
    "        topic_urls.append(base_url + tag['href'])\n",
    "        \n",
    "    return topic_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "55afa89c-c5df-4123-b988-1328c2bc7e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_topics():\n",
    "    \"\"\" Scrapes all GitHub topics and their metadata from the topics page.\n",
    "\n",
    "    Args:\n",
    "        None\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing topic titles, descriptions, and URLs. \n",
    "\n",
    "    Raises:\n",
    "        Exception: If the topics page fails to load. \n",
    "    \"\"\"\n",
    "    topics_url = 'https://github.com/topics'\n",
    "    response = requests.get(topics_url)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f'Failed to load page {topic_url}')\n",
    "    topics_dict = {\n",
    "        'title': get_topic_titles(doc), \n",
    "        'description': get_topic_descs(doc),\n",
    "        'url': get_topic_urls(doc)\n",
    "    }\n",
    "    return pd.DataFrame(topics_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "99a92eaa-93a0-43c0-999c-d184aab550f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_topics_repos():\n",
    "    \"\"\" Scrapes repositories from all topics on GitHub and saves them as .csv files. \n",
    "\n",
    "    Args:\n",
    "        None\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    print('Scraping list of topics:')\n",
    "    topics_df = scrape_topics()\n",
    "\n",
    "    os.makedirs('data_v3', exist_ok=True)\n",
    "    \n",
    "    for index, row in topics_df.iterrows():\n",
    "        print(f'Scraping top repos for topic \"{row['title']}\"')\n",
    "        scrape_topic(row['url'], 'data_v3/{}.csv'.format(row['title']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4fdd12ab-e9d0-4ab6-ab65-f74703e408d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping list of topics:\n",
      "Scraping top repos for topic \"3D\"\n",
      "Scraping top repos for topic \"Ajax\"\n",
      "Scraping top repos for topic \"Algorithm\"\n",
      "Scraping top repos for topic \"Amp\"\n",
      "Scraping top repos for topic \"Android\"\n",
      "Scraping top repos for topic \"Angular\"\n",
      "Scraping top repos for topic \"Ansible\"\n",
      "Scraping top repos for topic \"API\"\n",
      "Scraping top repos for topic \"Arduino\"\n",
      "Scraping top repos for topic \"ASP.NET\"\n",
      "Scraping top repos for topic \"Awesome Lists\"\n",
      "Scraping top repos for topic \"Amazon Web Services\"\n",
      "Scraping top repos for topic \"Azure\"\n",
      "Scraping top repos for topic \"Babel\"\n",
      "Scraping top repos for topic \"Bash\"\n",
      "Scraping top repos for topic \"Bitcoin\"\n",
      "Scraping top repos for topic \"Bootstrap\"\n",
      "Scraping top repos for topic \"Bot\"\n",
      "Scraping top repos for topic \"C\"\n",
      "Scraping top repos for topic \"Chrome\"\n",
      "Scraping top repos for topic \"Chrome extension\"\n",
      "Scraping top repos for topic \"Command-line interface\"\n",
      "Scraping top repos for topic \"Clojure\"\n",
      "Scraping top repos for topic \"Code quality\"\n",
      "Scraping top repos for topic \"Code review\"\n",
      "Scraping top repos for topic \"Compiler\"\n",
      "Scraping top repos for topic \"Continuous integration\"\n",
      "Scraping top repos for topic \"C++\"\n",
      "Scraping top repos for topic \"Cryptocurrency\"\n",
      "Scraping top repos for topic \"Crystal\"\n",
      "Scraping top repos for topic \"C#\"\n",
      "Scraping top repos for topic \"CSS\"\n",
      "Scraping top repos for topic \"Data structures\"\n",
      "Scraping top repos for topic \"Data visualization\"\n",
      "Scraping top repos for topic \"Database\"\n",
      "Scraping top repos for topic \"Deep learning\"\n",
      "Scraping top repos for topic \"Dependency management\"\n",
      "Scraping top repos for topic \"Deployment\"\n",
      "Scraping top repos for topic \"Django\"\n",
      "Scraping top repos for topic \"Docker\"\n",
      "Scraping top repos for topic \"Documentation\"\n",
      "Scraping top repos for topic \".NET\"\n",
      "Scraping top repos for topic \"Electron\"\n",
      "Scraping top repos for topic \"Elixir\"\n",
      "Scraping top repos for topic \"Emacs\"\n",
      "Scraping top repos for topic \"Ember\"\n",
      "Scraping top repos for topic \"Emoji\"\n",
      "Scraping top repos for topic \"Emulator\"\n",
      "Scraping top repos for topic \"ESLint\"\n",
      "Scraping top repos for topic \"Ethereum\"\n",
      "Scraping top repos for topic \"Express\"\n",
      "Scraping top repos for topic \"Firebase\"\n",
      "Scraping top repos for topic \"Firefox\"\n",
      "Scraping top repos for topic \"Flask\"\n",
      "Scraping top repos for topic \"Font\"\n",
      "Scraping top repos for topic \"Framework\"\n",
      "Scraping top repos for topic \"Front end\"\n",
      "Scraping top repos for topic \"Game engine\"\n",
      "Scraping top repos for topic \"Git\"\n",
      "Scraping top repos for topic \"GitHub API\"\n",
      "Scraping top repos for topic \"Go\"\n",
      "Scraping top repos for topic \"Google\"\n",
      "Scraping top repos for topic \"Gradle\"\n",
      "Scraping top repos for topic \"GraphQL\"\n",
      "Scraping top repos for topic \"Gulp\"\n",
      "Scraping top repos for topic \"Hacktoberfest\"\n",
      "Scraping top repos for topic \"Haskell\"\n",
      "Scraping top repos for topic \"Homebrew\"\n",
      "Scraping top repos for topic \"Homebridge\"\n",
      "Scraping top repos for topic \"HTML\"\n",
      "Scraping top repos for topic \"HTTP\"\n",
      "Scraping top repos for topic \"Icon font\"\n",
      "Scraping top repos for topic \"iOS\"\n",
      "Scraping top repos for topic \"IPFS\"\n",
      "Scraping top repos for topic \"Java\"\n",
      "Scraping top repos for topic \"JavaScript\"\n",
      "Scraping top repos for topic \"Jekyll\"\n",
      "Scraping top repos for topic \"jQuery\"\n",
      "Scraping top repos for topic \"JSON\"\n",
      "Scraping top repos for topic \"The Julia Language\"\n",
      "Scraping top repos for topic \"Jupyter Notebook\"\n",
      "Scraping top repos for topic \"Koa\"\n",
      "Scraping top repos for topic \"Kotlin\"\n",
      "Scraping top repos for topic \"Kubernetes\"\n",
      "Scraping top repos for topic \"Laravel\"\n",
      "Scraping top repos for topic \"LaTeX\"\n",
      "Scraping top repos for topic \"Library\"\n",
      "Scraping top repos for topic \"Linux\"\n",
      "Scraping top repos for topic \"Localization (l10n)\"\n",
      "Scraping top repos for topic \"Lua\"\n",
      "Scraping top repos for topic \"Machine learning\"\n",
      "Scraping top repos for topic \"macOS\"\n",
      "Scraping top repos for topic \"Markdown\"\n",
      "Scraping top repos for topic \"Mastodon\"\n",
      "Scraping top repos for topic \"Material Design\"\n",
      "Scraping top repos for topic \"MATLAB\"\n",
      "Scraping top repos for topic \"Maven\"\n",
      "Scraping top repos for topic \"Minecraft\"\n",
      "Scraping top repos for topic \"Mobile\"\n",
      "Scraping top repos for topic \"Monero\"\n",
      "Scraping top repos for topic \"MongoDB\"\n",
      "Scraping top repos for topic \"Mongoose\"\n",
      "Scraping top repos for topic \"Monitoring\"\n",
      "Scraping top repos for topic \"MvvmCross\"\n",
      "Scraping top repos for topic \"MySQL\"\n",
      "Scraping top repos for topic \"NativeScript\"\n",
      "Scraping top repos for topic \"Nim\"\n",
      "Scraping top repos for topic \"Natural language processing\"\n",
      "Scraping top repos for topic \"Node.js\"\n",
      "Scraping top repos for topic \"NoSQL\"\n",
      "Scraping top repos for topic \"npm\"\n",
      "Scraping top repos for topic \"Objective-C\"\n",
      "Scraping top repos for topic \"OpenGL\"\n",
      "Scraping top repos for topic \"Operating system\"\n",
      "Scraping top repos for topic \"P2P\"\n",
      "Scraping top repos for topic \"Package manager\"\n",
      "Scraping top repos for topic \"Parsing\"\n",
      "Scraping top repos for topic \"Perl\"\n",
      "Scraping top repos for topic \"Phaser\"\n",
      "Scraping top repos for topic \"PHP\"\n",
      "Scraping top repos for topic \"PICO-8\"\n",
      "Scraping top repos for topic \"Pixel Art\"\n",
      "Scraping top repos for topic \"PostgreSQL\"\n",
      "Scraping top repos for topic \"Project management\"\n",
      "Scraping top repos for topic \"Publishing\"\n",
      "Scraping top repos for topic \"PWA\"\n",
      "Scraping top repos for topic \"Python\"\n",
      "Scraping top repos for topic \"Qt\"\n",
      "Scraping top repos for topic \"R\"\n",
      "Scraping top repos for topic \"Rails\"\n",
      "Scraping top repos for topic \"Raspberry Pi\"\n",
      "Scraping top repos for topic \"Ratchet\"\n",
      "Scraping top repos for topic \"React\"\n",
      "Scraping top repos for topic \"React Native\"\n",
      "Scraping top repos for topic \"ReactiveUI\"\n",
      "Scraping top repos for topic \"Redux\"\n",
      "Scraping top repos for topic \"REST API\"\n",
      "Scraping top repos for topic \"Ruby\"\n",
      "Scraping top repos for topic \"Rust\"\n",
      "Scraping top repos for topic \"Sass\"\n",
      "Scraping top repos for topic \"Scala\"\n",
      "Scraping top repos for topic \"scikit-learn\"\n",
      "Scraping top repos for topic \"Software-defined networking\"\n",
      "Scraping top repos for topic \"Security\"\n",
      "Scraping top repos for topic \"Server\"\n",
      "Scraping top repos for topic \"Serverless\"\n",
      "Scraping top repos for topic \"Shell\"\n",
      "Scraping top repos for topic \"Sketch\"\n",
      "Scraping top repos for topic \"SpaceVim\"\n",
      "Scraping top repos for topic \"Spring Boot\"\n"
     ]
    }
   ],
   "source": [
    "# Starts the web scraping process for all topics and repositories \n",
    "scrape_topics_repos()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
