{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ff4a42c-4bad-4e36-b437-920906e23082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: pip\n",
      "zsh:1: command not found: pip\n",
      "zsh:1: command not found: pip\n",
      "zsh:1: command not found: pip\n"
     ]
    }
   ],
   "source": [
    "# Install all necessary libraries, update if necessary\n",
    "!pip install requests --upgrade --quiet\n",
    "!pip install beautifulsoup4 --upgrade --quiet\n",
    "!pip install pandas --upgrade --quiet\n",
    "!pip install selenium --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fae78c50-8f52-44b8-82ef-6188894543e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import os\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f6fa32d-0e59-4c0c-99a5-2a0f0ef7f840",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_url = 'https://github.com/topics' # Website we are scraping\n",
    "base_url = 'https://github.com' # Base URL we will add website extention to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28df3846-fe16-4883-9134-c9ef7b18c5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Safari() # Initialize Safari Driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66ede17b-07fe-471a-8f05-1853049bbc53",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Driver opens topics page\n",
    "    driver.get(topics_url)\n",
    "\n",
    "    for i in range(5):\n",
    "        # Driver waits until the buttom element is found\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.XPATH, \"//button[contains(@class, 'ajax-pagination-btn')]\"))\n",
    "        )\n",
    "        \n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    \n",
    "        # Wait until the button element is clickable\n",
    "        button_element = WebDriverWait(driver, 10).until(\n",
    "            EC.element_to_be_clickable((By.XPATH, \"//button[contains(@class, 'ajax-pagination-btn')]\"))\n",
    "        )\n",
    "\n",
    "        # Click the button\n",
    "        button_element.click()\n",
    "        time.sleep(1)\n",
    "\n",
    "    # Store updated html file\n",
    "    page_contents = driver.page_source\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "\n",
    "finally:\n",
    "    driver.quit()  # Always close the driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "988a664b-ef0b-4070-af9f-86a571d6af98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse page contents using the html parser in Beautiful Soup\n",
    "doc = BeautifulSoup(page_contents, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f8c21b3-876e-4f33-85db-a65f60ec7a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_star_count(stars_str):\n",
    "    \"\"\" Parses a star count string and converts it to an integer.\n",
    "\n",
    "    This function takes a string representing a star count, which may include a 'k' suffix\n",
    "    to denote thousands (e.g., .145k'). If the 'k' is present, it converts the numeric\n",
    "    portion to a float, multiplies by 1000, and returns the integer value. If no 'k' is\n",
    "    present, it directly converts the string to an integer. \n",
    "\n",
    "    Args: \n",
    "        stars_str (str): The star count as a string (e.g., '145k', '300').\n",
    "\n",
    "    Returns:\n",
    "        int: The numerical value of the star count (e.g., 145000, 300). \n",
    "    \"\"\"\n",
    "    stars_str = stars_str.strip()\n",
    "    if stars_str[-1] == 'k':\n",
    "        return int(float(stars_str[:-1]) * 1000)\n",
    "    return int(stars_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1cf6aa28-23e2-428e-882e-05eeb4095a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_page(topic_url):\n",
    "    \"\"\" Fetches and parses the HTML document of a GitHub topic page.\n",
    "\n",
    "    Args: \n",
    "        topic_url (str): The URL of the GitHub topic page.  \n",
    "\n",
    "    Returns:\n",
    "        BeautifulSoup: Parsed HTML document of the topic page.\n",
    "\n",
    "    Raises: \n",
    "        Exception: If the page fails to load. \n",
    "    \"\"\"\n",
    "    # Downlaod the page \n",
    "    response = requests.get(topic_url)\n",
    "    \n",
    "    # Check successful response\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f'Failed to load page {topic_url}')\n",
    "        \n",
    "    # Parse using Beautiful Soup\n",
    "    topic_doc = BeautifulSoup(response.text, 'html.parser')\n",
    "    return topic_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "720a5b6d-62b3-4ae0-b82e-3d59af0ad955",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_repo_info(h1_tag, star_tag):\n",
    "    \"\"\" Extracts repository information from the topic page.\n",
    "\n",
    "    Args:\n",
    "        h1_tag (BeautifulSoup tag): The h1 tag that contains the repository details. \n",
    "        star_tag (BeautifulSoup tag): The span tag that contains the star count.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (username, repository name, star count, repository URL).\n",
    "    \"\"\"\n",
    "    a_tags = h1_tag.find_all('a')\n",
    "    username = a_tags[0].text.strip()\n",
    "    repo_name = a_tags[1].text.strip()\n",
    "    repo_url = base_url + a_tags[1]['href']\n",
    "    stars = parse_star_count(star_tag.text.strip())\n",
    "    \n",
    "    return username, repo_name, stars, repo_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d66d898a-22fd-4e44-9b22-d80ca0fd1061",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_repos(topic_doc):\n",
    "    \"\"\" Extracts repository details from a GitHub topic page. \n",
    "\n",
    "    Args:\n",
    "        topic_doc (BeautifulSoup): Parsed the HTML document of the topic page.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing repository details (username, name, stars, URL).\n",
    "    \"\"\"\n",
    "    # Get the h1 tags containing repo title, repo URL, and username\n",
    "    h1_selection_class = 'f3 color-fg-muted text-normal lh-condensed'\n",
    "    repo_tags = topic_doc.find_all('h3', {'class': h1_selection_class})\n",
    "    \n",
    "    # Get star tags\n",
    "    star_tags = topic_doc.find_all('span', {'class': 'Counter js-social-count'})\n",
    "\n",
    "    topic_repos_dict = {\n",
    "        'username' : [],\n",
    "        'repo_name' : [],\n",
    "        'forks' : [],\n",
    "        'stars': [],\n",
    "        'repo_url' : []\n",
    "    }\n",
    "    \n",
    "    # Get repo info\n",
    "    for i in range(len(repo_tags)):\n",
    "        repo_info = get_repo_info(repo_tags[i], star_tags[i])\n",
    "        topic_repos_dict['username'].append(repo_info[0])\n",
    "        topic_repos_dict['repo_name'].append(repo_info[1])\n",
    "        topic_repos_dict['forks'].append(repo_info[2])\n",
    "        topic_repos_dict['stars'].append(repo_info[3])\n",
    "        topic_repos_dict['repo_url'].append(repo_info[4])\n",
    "\n",
    "    return pd.DataFrame(topic_repos_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b5d503ef-55aa-4e69-a685-2b6299983d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_topic(topic_url, path):\n",
    "    \"\"\" Scrapes repositories from a GitHub topic page and saves them as a .csv file. \n",
    "\n",
    "    Args:\n",
    "    topic_url (str): The URL of the GitHub topic page. \n",
    "    path (str): The file path to save the scraped data. \n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    if os.path.exists(path):\n",
    "        print(f\"The file {path} already exists. Skipping...\")\n",
    "        return\n",
    "    topic_df = get_topic_repos(get_topic_page(topic_url))\n",
    "    \n",
    "    topic_df.to_csv(path, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "25c8ecf2-1f21-493d-a33d-3a0eb421be1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_titles(doc):\n",
    "    \"\"\" Extracts topic titles from the GitHub topics page.\n",
    "\n",
    "    Args:\n",
    "        doc (BeautifulSoup): Parsed HTML document of the topics page. \n",
    "\n",
    "    Returns:\n",
    "        list: A list of topic titles.\n",
    "    \"\"\"\n",
    "    selection_class = 'f3 lh-condensed mb-0 mt-1 Link--primary'\n",
    "    topic_title_tags = doc.find_all('p', {'class' : selection_class})\n",
    "    topic_titles = []\n",
    "    for tag in topic_title_tags:\n",
    "        topic_titles.append(tag.text)\n",
    "    return topic_titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a4cae779-d3f6-42b7-96aa-549512d7e6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_descs(doc):\n",
    "    \"\"\" Extracts the topic descriptions from the GitHub topics page.\n",
    "\n",
    "    Args:\n",
    "        doc (BeautifulSoup): Parsed HTML document of the topics page.\n",
    "        \n",
    "    Returns:\n",
    "        list: A list of topic descriptions. \n",
    "    \"\"\"\n",
    "    desc_selector = 'f5 color-fg-muted mb-0 mt-1'\n",
    "    topic_desc_tags = doc.find_all('p', {'class', desc_selector})\n",
    "    \n",
    "    topic_descs = []\n",
    "    for tag in topic_desc_tags:\n",
    "        topic_descs.append(tag.text.strip())\n",
    "    return topic_descs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "40db1921-eb04-4a0d-9fc0-5b4b5807f06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_urls(doc):\n",
    "    \"\"\" Extracts topic URLs from the GitHub topics page. \n",
    "\n",
    "    Args:\n",
    "        doc (BeautifulSoup): Parsed HTML document of the topics page.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of GitHub topic URLs.\n",
    "    \"\"\"\n",
    "    topic_link_tags = doc.find_all('a', {'class': 'no-underline flex-1 d-flex flex-column'})\n",
    "    topic_urls = []\n",
    "    base_url = 'https://github.com'\n",
    "    \n",
    "    for tag in topic_link_tags:\n",
    "        topic_urls.append(base_url + tag['href'])\n",
    "        \n",
    "    return topic_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "55afa89c-c5df-4123-b988-1328c2bc7e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_topics():\n",
    "    \"\"\" Scrapes all GitHub topics and their metadata from the topics page.\n",
    "\n",
    "    Args:\n",
    "        None\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing topic titles, descriptions, and URLs. \n",
    "\n",
    "    Raises:\n",
    "        Exception: If the topics page fails to load. \n",
    "    \"\"\"\n",
    "    topics_url = 'https://github.com/topics'\n",
    "    response = requests.get(topics_url)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f'Failed to load page {topic_url}')\n",
    "    topics_dict = {\n",
    "        'title': get_topic_titles(doc), \n",
    "        'description': get_topic_descs(doc),\n",
    "        'url': get_topic_urls(doc)\n",
    "    }\n",
    "    return pd.DataFrame(topics_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "99a92eaa-93a0-43c0-999c-d184aab550f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_topics_repos():\n",
    "    \"\"\" Scrapes repositories from all topics on GitHub and saves them as .csv files. \n",
    "\n",
    "    Args:\n",
    "        None\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    print('Scraping list of topics:')\n",
    "    topics_df = scrape_topics()\n",
    "\n",
    "    os.makedirs('data_v4', exist_ok=True)\n",
    "    \n",
    "    for index, row in topics_df.iterrows():\n",
    "        print(f'Scraping top repos for topic \"{row['title']}\"')\n",
    "        scrape_topic(row['url'], 'data_v4/{}.csv'.format(row['title']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4fdd12ab-e9d0-4ab6-ab65-f74703e408d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping list of topics:\n",
      "Scraping top repos for topic \"3D\"\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Starts the web scraping process for all topics and repositories \u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mscrape_topics_repos\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[16], line 17\u001b[0m, in \u001b[0;36mscrape_topics_repos\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index, row \u001b[38;5;129;01min\u001b[39;00m topics_df\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mScraping top repos for topic \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrow[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 17\u001b[0m     \u001b[43mscrape_topic\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43murl\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata_v4/\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtitle\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[11], line 14\u001b[0m, in \u001b[0;36mscrape_topic\u001b[0;34m(topic_url, path)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m already exists. Skipping...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m topic_df \u001b[38;5;241m=\u001b[39m \u001b[43mget_topic_repos\u001b[49m\u001b[43m(\u001b[49m\u001b[43mget_topic_page\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtopic_url\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m topic_df\u001b[38;5;241m.\u001b[39mto_csv(path, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[10], line 32\u001b[0m, in \u001b[0;36mget_topic_repos\u001b[0;34m(topic_doc)\u001b[0m\n\u001b[1;32m     30\u001b[0m     topic_repos_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mforks\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(repo_info[\u001b[38;5;241m2\u001b[39m])\n\u001b[1;32m     31\u001b[0m     topic_repos_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstars\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(repo_info[\u001b[38;5;241m3\u001b[39m])\n\u001b[0;32m---> 32\u001b[0m     topic_repos_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrepo_url\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(\u001b[43mrepo_info\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mDataFrame(topic_repos_dict)\n",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "# Starts the web scraping process for all topics and repositories \n",
    "scrape_topics_repos()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
