{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ff4a42c-4bad-4e36-b437-920906e23082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: pip\n",
      "zsh:1: command not found: pip\n",
      "zsh:1: command not found: pip\n"
     ]
    }
   ],
   "source": [
    "# Install all necessary libraries, update if necessary\n",
    "!pip install requests --upgrade --quiet\n",
    "!pip install beautifulsoup4 --upgrade --quiet\n",
    "!pip install pandas --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fae78c50-8f52-44b8-82ef-6188894543e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f6fa32d-0e59-4c0c-99a5-2a0f0ef7f840",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_url = 'https://github.com/topics' # Website we are scraping\n",
    "base_url = 'https://github.com' # Base URL we will add website extention to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45569ac4-ae36-4b20-93b5-42f3fc66ec27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve data from topic_url\n",
    "response = requests.get(topics_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d77bf94-c3c5-4ed0-b503-b8733c87b725",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieves content from topics_url as a string\n",
    "page_contents = response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "988a664b-ef0b-4070-af9f-86a571d6af98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse page contents using the html parser in Beautiful Soup\n",
    "doc = BeautifulSoup(page_contents, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f8c21b3-876e-4f33-85db-a65f60ec7a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_star_count(stars_str):\n",
    "    \"\"\" Parses a star count string and converts it to an integer.\n",
    "\n",
    "    This function takes a string representing a star count, which may include a 'k' suffix\n",
    "    to denote thousands (e.g., .145k'). If the 'k' is present, it converts the numeric\n",
    "    portion to a float, multiplies by 1000, and returns the integer value. If no 'k' is\n",
    "    present, it directly converts the string to an integer. \n",
    "\n",
    "    Args: \n",
    "        stars_str (str): The star count as a string (e.g., '145k', '300').\n",
    "\n",
    "    Returns:\n",
    "        int: The numerical value of the star count. \n",
    "    \"\"\"\n",
    "    stars_str = stars_str.strip()\n",
    "    if stars_str[-1] == 'k':\n",
    "        return int(float(stars_str[:-1]) * 1000)\n",
    "    return int(stars_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1cf6aa28-23e2-428e-882e-05eeb4095a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_page(topic_url):\n",
    "    \"\"\" Fetches and parses the HTML document of a GitHub topic page.\n",
    "\n",
    "    Args: \n",
    "        topic_url (str): The URL of the GitHub topic page.  \n",
    "\n",
    "    Returns:\n",
    "        BeautifulSoup: Parsed HTML document of the topic page.\n",
    "\n",
    "    Raises: \n",
    "        Exception: If the page fails to load. \n",
    "    \"\"\"\n",
    "    # Downlaod the page \n",
    "    response = requests.get(topic_url)\n",
    "    \n",
    "    # Check successful response\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f'Failed to load page {topic_url}')\n",
    "        \n",
    "    # Parse using Beautiful Soup\n",
    "    topic_doc = BeautifulSoup(response.text, 'html.parser')\n",
    "    return topic_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "720a5b6d-62b3-4ae0-b82e-3d59af0ad955",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_repo_info(h1_tag, star_tag):\n",
    "    \"\"\" Extracts repository information from the topic page.\n",
    "\n",
    "    Args:\n",
    "        h1_tag (BeautifulSoup tag): The h1 tag that contains the repository details. \n",
    "        star_tag (BeautifulSoup tag): The span tag that contains the star count.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (username, repository name, star count, repository URL).\n",
    "    \"\"\"\n",
    "    a_tags = h1_tag.find_all('a')\n",
    "    username = a_tags[0].text.strip()\n",
    "    repo_name = a_tags[1].text.strip()\n",
    "    repo_url = base_url + a_tags[1]['href']\n",
    "    stars = parse_star_count(star_tag.text.strip())\n",
    "    \n",
    "    return username, repo_name, stars, repo_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d66d898a-22fd-4e44-9b22-d80ca0fd1061",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_repos(topic_doc):\n",
    "    \"\"\" Extracts repository details from a GitHub topic page. \n",
    "\n",
    "    Args:\n",
    "        topic_doc (BeautifulSoup): Parsed the HTML document of the topic page.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing repository details (username, name, stars, URL).\n",
    "    \"\"\"\n",
    "    # Get the h1 tags containing repo title, repo URL, and username\n",
    "    h1_selection_class = 'f3 color-fg-muted text-normal lh-condensed'\n",
    "    repo_tags = topic_doc.find_all('h3', {'class': h1_selection_class})\n",
    "    \n",
    "    # Get star tags\n",
    "    star_tags = topic_doc.find_all('span', {'class': 'Counter js-social-count'})\n",
    "\n",
    "    topic_repos_dict = {\n",
    "        'username' : [],\n",
    "        'repo_name' : [],\n",
    "        'stars': [],\n",
    "        'repo_url' : []\n",
    "    }\n",
    "    \n",
    "    # Get repo info\n",
    "    for i in range(len(repo_tags)):\n",
    "        repo_info = get_repo_info(repo_tags[i], star_tags[i])\n",
    "        topic_repos_dict['username'].append(repo_info[0])\n",
    "        topic_repos_dict['repo_name'].append(repo_info[1])\n",
    "        topic_repos_dict['stars'].append(repo_info[2])\n",
    "        topic_repos_dict['repo_url'].append(repo_info[3])\n",
    "\n",
    "    return pd.DataFrame(topic_repos_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b5d503ef-55aa-4e69-a685-2b6299983d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_topic(topic_url, path):\n",
    "    \"\"\" Scrapes repositories from a GitHub topic page and saves them as a .csv file. \n",
    "\n",
    "    Args:\n",
    "    topic_url (str): The URL of the GitHub topic page. \n",
    "    path (str): The file path to save the scraped data. \n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    if os.path.exists(path):\n",
    "        print(f\"The file {path} already exists. Skipping...\")\n",
    "        return\n",
    "    topic_df = get_topic_repos(get_topic_page(topic_url))\n",
    "    \n",
    "    topic_df.to_csv(path, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "25c8ecf2-1f21-493d-a33d-3a0eb421be1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_titles(doc):\n",
    "    \"\"\" Extracts topic titles from the GitHub topics page.\n",
    "\n",
    "    Args:\n",
    "        doc (BeautifulSoup): Parsed HTML document of the topics page. \n",
    "\n",
    "    Returns:\n",
    "        list: A list of topic titles.\n",
    "    \"\"\"\n",
    "    selection_class = 'f3 lh-condensed mb-0 mt-1 Link--primary'\n",
    "    topic_title_tags = doc.find_all('p', {'class' : selection_class})\n",
    "    topic_titles = []\n",
    "    for tag in topic_title_tags:\n",
    "        topic_titles.append(tag.text)\n",
    "    return topic_titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a4cae779-d3f6-42b7-96aa-549512d7e6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_descs(doc):\n",
    "    \"\"\" Extracts the topic descriptions from the GitHub topics page.\n",
    "\n",
    "    Args:\n",
    "        doc (BeautifulSoup): Parsed HTML document of the topics page.\n",
    "        \n",
    "    Returns:\n",
    "        list: A list of topic descriptions. \n",
    "    \"\"\"\n",
    "    desc_selector = 'f5 color-fg-muted mb-0 mt-1'\n",
    "    topic_desc_tags = doc.find_all('p', {'class', desc_selector})\n",
    "    \n",
    "    topic_descs = []\n",
    "    for tag in topic_desc_tags:\n",
    "        topic_descs.append(tag.text.strip())\n",
    "    return topic_descs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "40db1921-eb04-4a0d-9fc0-5b4b5807f06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_urls(doc):\n",
    "    \"\"\" Extracts topic URLs from the GitHub topics page. \n",
    "\n",
    "    Args:\n",
    "        doc (BeautifulSoup): Parsed HTML document of the topics page.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of GitHub topic URLs.\n",
    "    \"\"\"\n",
    "    topic_link_tags = doc.find_all('a', {'class': 'no-underline flex-1 d-flex flex-column'})\n",
    "    topic_urls = []\n",
    "    base_url = 'https://github.com'\n",
    "    \n",
    "    for tag in topic_link_tags:\n",
    "        topic_urls.append(base_url + tag['href'])\n",
    "        \n",
    "    return topic_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "55afa89c-c5df-4123-b988-1328c2bc7e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_topics():\n",
    "    \"\"\" Scrapes all GitHub topics and their metadata from the topics page.\n",
    "\n",
    "    Args:\n",
    "        None\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing topic titles, descriptions, and URLs. \n",
    "\n",
    "    Raises:\n",
    "        Exception: If the topics page fails to load. \n",
    "    \"\"\"\n",
    "    topics_url = 'https://github.com/topics'\n",
    "    response = requests.get(topics_url)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f'Failed to load page {topic_url}')\n",
    "    topics_dict = {\n",
    "        'title': get_topic_titles(doc), \n",
    "        'description': get_topic_descs(doc),\n",
    "        'url': get_topic_urls(doc)\n",
    "    }\n",
    "    return pd.DataFrame(topics_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "99a92eaa-93a0-43c0-999c-d184aab550f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_topics_repos():\n",
    "    \"\"\" Scrapes repositories from all topics on GitHub and saves them as .csv files. \n",
    "\n",
    "    Args:\n",
    "        None\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    print('Scraping list of topics:')\n",
    "    topics_df = scrape_topics()\n",
    "\n",
    "    os.makedirs('data_v2', exist_ok=True)\n",
    "    \n",
    "    for index, row in topics_df.iterrows():\n",
    "        print(f'Scraping top repos for \"{row['title']}\"')\n",
    "        scrape_topic(row['url'], 'data_v2/{}.csv'.format(row['title']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4fdd12ab-e9d0-4ab6-ab65-f74703e408d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping top repos for \"Amp\"\n",
      "Scraping top repos for \"Android\"\n",
      "Scraping top repos for \"Angular\"\n",
      "Scraping top repos for \"Ansible\"\n",
      "Scraping top repos for \"API\"\n",
      "Scraping top repos for \"Arduino\"\n",
      "Scraping top repos for \"ASP.NET\"\n",
      "Scraping top repos for \"Awesome Lists\"\n",
      "Scraping top repos for \"Amazon Web Services\"\n",
      "Scraping top repos for \"Azure\"\n",
      "Scraping top repos for \"Babel\"\n",
      "Scraping top repos for \"Bash\"\n",
      "Scraping top repos for \"Bitcoin\"\n",
      "Scraping top repos for \"Bootstrap\"\n",
      "Scraping top repos for \"Bot\"\n",
      "Scraping top repos for \"C\"\n",
      "Scraping top repos for \"Chrome\"\n",
      "Scraping top repos for \"Chrome extension\"\n",
      "Scraping top repos for \"Command-line interface\"\n",
      "Scraping top repos for \"Clojure\"\n",
      "Scraping top repos for \"Code quality\"\n",
      "Scraping top repos for \"Code review\"\n",
      "Scraping top repos for \"Compiler\"\n",
      "Scraping top repos for \"Continuous integration\"\n",
      "Scraping top repos for \"C++\"\n",
      "Scraping top repos for \"Cryptocurrency\"\n",
      "Scraping top repos for \"Crystal\"\n"
     ]
    }
   ],
   "source": [
    "# Starts the web scraping process for all topics and repositories \n",
    "scrape_topics_repos()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
